{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "    Feature selection is an important process in the workflow of Machine Learning. It involves selecting the most important/relevant features for better prediction of the Target variable.\n",
    "    \n",
    "    \n",
    "    Following are feature selection operations performed on iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# create a base classifier used to evaluate a subset of attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination\n",
    "\n",
    "\n",
    "    This is a feature elimination technique where in some attributes are removed and model is built. From the models built from elimination of one or more models, the features that gives good accuracy is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True  True]\n",
      "[2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "model = LogisticRegression()\n",
    "# create the RFE model and select 3 attributes\n",
    "rfe = RFE(model, 3)\n",
    "rfe = rfe.fit(dataset.data, dataset.target)\n",
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sepal length (cm)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.feature_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Here we choose 3 of 4 features. And we find that the last three features are important than the fourth (sepal length). Lets now construct a feature with all features and with these three features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression with rfe data \n",
    "# Choosing three features from the dataset\n",
    "rfe_data = dataset.data[:,rfe.support_]\n",
    "\n",
    "lr2=LogisticRegression()\n",
    "X_train,X_test,y_train,y_test=train_test_split(rfe_data,dataset.target,random_state=0)\n",
    "lr2.fit(X_train,y_train)\n",
    "y_pred=lr2.predict(X_test)\n",
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.868421052631579"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression with whole data\n",
    "lr1=LogisticRegression()\n",
    "X_train,X_test,y_train,y_test=train_test_split(dataset.data,dataset.target,random_state=0)\n",
    "lr1.fit(X_train,y_train)\n",
    "y_pred=lr1.predict(X_test)\n",
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Here we see that there is an improvement in accuracy with the feature selection. However this need always be true. Our aim was to reduce the dimension and it happened!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded Methods\n",
    "\n",
    "    We can use regularizations like L1 and L2 and tree based models like Randomforest,ExtraTree etc.,  to find the feature importance.\n",
    "    \n",
    "    In this session we will first perform feature selection with Lasso, and then with random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso with linear support vector classifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC(C=0.01,penalty='l1',dual=False)\n",
    "lsvc.fit(X_train,y_train)\n",
    "feat = SelectFromModel(lsvc,prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, False])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Here we find that the first three features are important, so the fourth parameter is the least important! ( as chosen by l1 regularization with svc). At this point lets find the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "feat = SelectFromModel(rf,prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True,  True])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here the last two parameters are chosen. We can also obtain the level of importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18577196, 0.03136508, 0.46515608, 0.31770689])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Here we see that the last two features has more importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally\n",
    "\n",
    "    Now we see that there is a strong corrlation between feature selection and model selection. We have to select features based on the model selections. Also Domain knowledge is very important when it comes to feature selection.\n",
    "    \n",
    "    \n",
    "    Feature selection can remove irrelevant features, resulting in reduction of efforts to collect data. It also reduces the complexity of the problem and time and computation needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Reference: \n",
    "   1. https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/\n",
    "   2. https://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
